from transformers import AutoModelForCausalLM, AutoTokenizer
import os

# Cr√©er le dossier models/ s'il n'existe pas
os.makedirs("models/TinyLlama-1.1B-Chat-v1.0", exist_ok=True)

# Nom du mod√®le
model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

print(f"üîç T√©l√©chargement du mod√®le {model_name}...")

# T√©l√©charger et sauvegarder le tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.save_pretrained("models/TinyLlama-1.1B-Chat-v1.0")
print("‚úÖ Tokenizer sauvegard√©.")

# T√©l√©charger et sauvegarder le mod√®le
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="cpu",  # T√©l√©charge sur CPU (pas besoin de GPU pour le t√©l√©chargement)
    low_cpu_mem_usage=True  # R√©duit la m√©moire utilis√©e pendant le t√©l√©chargement
)
model.save_pretrained("models/TinyLlama-1.1B-Chat-v1.0")
print("‚úÖ Mod√®le sauvegard√© dans models/TinyLlama-1.1B-Chat-v1.0/")

print("üéâ T√©l√©chargement termin√© ! Tu peux maintenant utiliser le mod√®le localement.")
